{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gerba\\AppData\\Local\\Temp\\ipykernel_37128\\1130341577.py:36: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  x_train = torch.sparse_coo_tensor(x_train.nonzero(), x_train.data, x_train.shape, device=device).float()\n",
      "c:\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7930218805440568\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the processed data\n",
    "combined_sparse = sp.load_npz('data/combined_sparse.npz')\n",
    "df_targets = pd.read_csv('data/df_targets.csv')\n",
    "df_targets['PRIM_CONTRIBUTORY_CAUSE'] = pd.Categorical(df_targets['PRIM_CONTRIBUTORY_CAUSE'])\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df_targets_encoded = pd.DataFrame()\n",
    "df_targets_encoded['PRIM_CONTRIBUTORY_CAUSE_LABEL'] = label_encoder.fit_transform(df_targets['PRIM_CONTRIBUTORY_CAUSE'].cat.codes)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    combined_sparse,\n",
    "    df_targets_encoded['PRIM_CONTRIBUTORY_CAUSE_LABEL'].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert the target arrays to PyTorch tensors\n",
    "y_train_tensor = torch.tensor(y_train).long()\n",
    "y_test_tensor = torch.tensor(y_test).long()\n",
    "\n",
    "# Set the device to use for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Move the data to the device\n",
    "x_train = torch.sparse_coo_tensor(x_train.nonzero(), x_train.data, x_train.shape, device=device).float()\n",
    "x_test = torch.sparse_coo_tensor(x_test.nonzero(), x_test.data, x_test.shape, device=device).float()\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "y_test_tensor = y_test_tensor.to(device)\n",
    "\n",
    "# Create the training dataset and data loader\n",
    "train_dataset = TensorDataset(x_train, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define the neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Set the hyperparameters\n",
    "input_size = combined_sparse.shape[1]\n",
    "hidden_size = 128\n",
    "num_classes = len(label_encoder.classes_)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# Create an instance of the neural network model\n",
    "model = NeuralNetwork(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_data, batch_targets in train_loader:\n",
    "        # Move the batch data and targets to the device\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(batch_data)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(logits, batch_targets)\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Forward pass\n",
    "    logits = model(x_test)\n",
    "\n",
    "    # Get the predicted labels\n",
    "    _, predicted = torch.max(logits.data, 1)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::as_strided' with arguments from the 'SparseCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCPU.cpp:31115 [kernel]\nCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCUDA.cpp:44031 [kernel]\nMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterMeta.cpp:26824 [kernel]\nQuantizedCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:492 [backend fallback]\nFunctionalize: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterFunctionalization_0.cpp:21506 [kernel]\nNamed: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\NegateFallback.cpp:23 [kernel]\nZeroTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterZeroTensor.cpp:161 [kernel]\nADInplaceOrView: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\ADInplaceOrViewType_0.cpp:4719 [kernel]\nAutogradOther: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradHIP: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradXLA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradIPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradVE: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradLazy: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradMTIA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradPrivateUse1: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradPrivateUse2: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradPrivateUse3: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradNestedTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nTracer: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\TraceType_0.cpp:16759 [kernel]\nAutocastCPU: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:345 [backend fallback]\nAutocastCUDA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:212 [backend fallback]\nFuncTorchBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:714 [kernel]\nFuncTorchVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1079 [kernel]\nVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:219 [backend fallback]\nPythonTLSSnapshot: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:488 [backend fallback]\nPythonDispatcher: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:148 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m x_test_copy \u001b[39m=\u001b[39m x_test\u001b[39m.\u001b[39mclone()\n\u001b[0;32m      9\u001b[0m \u001b[39m# Define the range of values for the chosen feature\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m feature_values \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlinspace(torch\u001b[39m.\u001b[39mmin(x_test_copy[:, feature_index]), torch\u001b[39m.\u001b[39mmax(x_test_copy[:, feature_index]), steps\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[39m# Set the chosen feature to the specified values while keeping the other features unchanged\u001b[39;00m\n\u001b[0;32m     13\u001b[0m x_test_copy[:, feature_index] \u001b[39m=\u001b[39m feature_values\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Could not run 'aten::as_strided' with arguments from the 'SparseCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCPU.cpp:31115 [kernel]\nCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCUDA.cpp:44031 [kernel]\nMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterMeta.cpp:26824 [kernel]\nQuantizedCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:492 [backend fallback]\nFunctionalize: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterFunctionalization_0.cpp:21506 [kernel]\nNamed: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\NegateFallback.cpp:23 [kernel]\nZeroTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterZeroTensor.cpp:161 [kernel]\nADInplaceOrView: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\ADInplaceOrViewType_0.cpp:4719 [kernel]\nAutogradOther: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradHIP: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradXLA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradIPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradVE: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradLazy: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradMTIA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradPrivateUse1: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradPrivateUse2: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradPrivateUse3: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nAutogradNestedTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16430 [autograd kernel]\nTracer: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\TraceType_0.cpp:16759 [kernel]\nAutocastCPU: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:345 [backend fallback]\nAutocastCUDA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:212 [backend fallback]\nFuncTorchBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:714 [kernel]\nFuncTorchVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1079 [kernel]\nVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:219 [backend fallback]\nPythonTLSSnapshot: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:488 [backend fallback]\nPythonDispatcher: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:148 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose a feature index to analyze (e.g., 0 for the first feature)\n",
    "feature_index = 0\n",
    "\n",
    "# Create a copy of the test dataset\n",
    "x_test_copy = x_test.clone()\n",
    "\n",
    "# Define the range of values for the chosen feature\n",
    "feature_values = torch.linspace(torch.min(x_test_copy[:, feature_index]), torch.max(x_test_copy[:, feature_index]), steps=100)\n",
    "\n",
    "# Set the chosen feature to the specified values while keeping the other features unchanged\n",
    "x_test_copy[:, feature_index] = feature_values\n",
    "\n",
    "# Forward pass with the modified dataset\n",
    "logits = model(x_test_copy)\n",
    "\n",
    "# Calculate the average predicted probability for each feature value\n",
    "average_predictions = torch.mean(torch.softmax(logits, dim=1), dim=0)\n",
    "\n",
    "# Plot the partial dependence plot\n",
    "plt.plot(feature_values.cpu().numpy(), average_predictions.cpu().numpy())\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Average Predicted Probability')\n",
    "plt.title('Partial Dependence Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\lib\\site-packages\\shap\\utils\\_clustering.py:35: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "c:\\Anaconda3\\lib\\site-packages\\shap\\utils\\_clustering.py:54: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "c:\\Anaconda3\\lib\\site-packages\\shap\\utils\\_clustering.py:63: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window(order, start, length):\n",
      "c:\\Anaconda3\\lib\\site-packages\\shap\\utils\\_clustering.py:69: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "c:\\Anaconda3\\lib\\site-packages\\shap\\utils\\_clustering.py:77: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _mask_delta_score(m1, m2):\n",
      "c:\\Anaconda3\\lib\\site-packages\\shap\\links.py:5: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def identity(x):\n",
      "c:\\Anaconda3\\lib\\site-packages\\shap\\links.py:10: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _identity_inverse(x):\n",
      "c:\\Anaconda3\\lib\\site-packages\\shap\\links.py:15: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def logit(x):\n",
      "c:\\Anaconda3\\lib\\site-packages\\shap\\links.py:20: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _logit_inverse(x):\n",
      "c:\\Anaconda3\\lib\\site-packages\\shap\\utils\\_masked_model.py:363: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "c:\\Anaconda3\\lib\\site-packages\\shap\\utils\\_masked_model.py:385: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "c:\\Anaconda3\\lib\\site-packages\\shap\\utils\\_masked_model.py:428: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
      "c:\\Anaconda3\\lib\\site-packages\\shap\\utils\\_masked_model.py:439: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
      "c:\\Anaconda3\\lib\\site-packages\\shap\\maskers\\_tabular.py:186: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "c:\\Anaconda3\\lib\\site-packages\\shap\\maskers\\_tabular.py:197: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "c:\\Anaconda3\\lib\\site-packages\\shap\\maskers\\_image.py:175: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
      "c:\\Anaconda3\\lib\\site-packages\\shap\\explainers\\_partition.py:676: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def lower_credit(i, value, M, values, clustering):\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'warnings' has no attribute 'DeprecationWarning'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m explainer \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39mExplainer(model, x_train)\n\u001b[0;32m      6\u001b[0m \u001b[39m# Compute SHAP values for the test dataset\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m shap_values \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39;49mshap_values(x_test)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Summarize the SHAP values\u001b[39;00m\n\u001b[0;32m     10\u001b[0m shap\u001b[39m.\u001b[39msummary_plot(shap_values, feature_names\u001b[39m=\u001b[39mfeature_names, plot_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbar\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Anaconda3\\lib\\site-packages\\shap\\explainers\\_permutation.py:214\u001b[0m, in \u001b[0;36mPermutation.shap_values\u001b[1;34m(self, X, npermutations, main_effects, error_bounds, batch_evals, silent)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshap_values\u001b[39m(\u001b[39mself\u001b[39m, X, npermutations\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, main_effects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, error_bounds\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, batch_evals\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, silent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    191\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Legacy interface to estimate the SHAP values for a set of samples.\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39m        of such matrices, one for each output.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mshap_values() is deprecated; use __call__().\u001b[39m\u001b[39m\"\u001b[39m, warnings\u001b[39m.\u001b[39;49mDeprecationWarning)\n\u001b[0;32m    216\u001b[0m     explanation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(X, max_evals\u001b[39m=\u001b[39mnpermutations \u001b[39m*\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], main_effects\u001b[39m=\u001b[39mmain_effects)\n\u001b[0;32m    217\u001b[0m     \u001b[39mreturn\u001b[39;00m explanation\u001b[39m.\u001b[39mvalues\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'warnings' has no attribute 'DeprecationWarning'"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "# Create an explainer object\n",
    "explainer = shap.Explainer(model, x_train)\n",
    "\n",
    "# Compute SHAP values for the test dataset\n",
    "shap_values = explainer.shap_values(x_test)\n",
    "\n",
    "# Summarize the SHAP values\n",
    "shap.summary_plot(shap_values, feature_names=feature_names, plot_type='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlime\u001b[39;00m \u001b[39mimport\u001b[39;00m lime_tabular\n\u001b[0;32m      3\u001b[0m \u001b[39m# Create an explainer object\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m explainer \u001b[39m=\u001b[39m lime_tabular\u001b[39m.\u001b[39mLimeTabularExplainer(x_train, feature_names\u001b[39m=\u001b[39mfeature_names)\n\u001b[0;32m      6\u001b[0m \u001b[39m# Choose a specific instance for explanation (e.g., the first instance in the test set)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m instance_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_names' is not defined"
     ]
    }
   ],
   "source": [
    "from lime import lime_tabular\n",
    "\n",
    "# Create an explainer object\n",
    "explainer = lime_tabular.LimeTabularExplainer(x_train, feature_names=feature_names)\n",
    "\n",
    "# Choose a specific instance for explanation (e.g., the first instance in the test set)\n",
    "instance_index = 0\n",
    "instance = x_test[instance_index].cpu().numpy()\n",
    "\n",
    "# Generate explanations for the chosen instance\n",
    "explanation = explainer.explain_instance(instance, model.predict_proba, num_features=len(feature_names))\n",
    "\n",
    "# Show the explanation\n",
    "explanation.show_in_notebook()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Older Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gerba\\AppData\\Local\\Temp\\ipykernel_31816\\2837531019.py:36: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  x_train = torch.sparse_coo_tensor(x_train.nonzero(), x_train.data, x_train.shape, device=device).float()\n",
      "c:\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7953625931621909\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import scipy.sparse as sp\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from torch import nn\n",
    "# from torch.optim import Adam\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Load the processed data\n",
    "# combined_sparse = sp.load_npz('data/combined_sparse.npz')\n",
    "# df_targets = pd.read_csv('data/df_targets.csv')\n",
    "# df_targets['PRIM_CONTRIBUTORY_CAUSE'] = pd.Categorical(df_targets['PRIM_CONTRIBUTORY_CAUSE'])\n",
    "\n",
    "# # Encode the target variable\n",
    "# label_encoder = LabelEncoder()\n",
    "# df_targets_encoded = pd.DataFrame()\n",
    "# df_targets_encoded['PRIM_CONTRIBUTORY_CAUSE_LABEL'] = label_encoder.fit_transform(df_targets['PRIM_CONTRIBUTORY_CAUSE'].cat.codes)\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#     combined_sparse,\n",
    "#     df_targets_encoded['PRIM_CONTRIBUTORY_CAUSE_LABEL'].values,\n",
    "#     test_size=0.2,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Convert the target arrays to PyTorch tensors\n",
    "# y_train_tensor = torch.tensor(y_train).long()\n",
    "# y_test_tensor = torch.tensor(y_test).long()\n",
    "\n",
    "# # Set the device to use for training\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)\n",
    "# # Move the data to the device\n",
    "# x_train = torch.sparse_coo_tensor(x_train.nonzero(), x_train.data, x_train.shape, device=device).float()\n",
    "# x_test = torch.sparse_coo_tensor(x_test.nonzero(), x_test.data, x_test.shape, device=device).float()\n",
    "# y_train_tensor = y_train_tensor.to(device)\n",
    "# y_test_tensor = y_test_tensor.to(device)\n",
    "\n",
    "# # Create the training dataset and data loader\n",
    "# train_dataset = TensorDataset(x_train, y_train_tensor)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# # Define the neural network model\n",
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_classes):\n",
    "#         super(NeuralNetwork, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.fc1(x)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "\n",
    "# # Set the hyperparameters\n",
    "# input_size = combined_sparse.shape[1]\n",
    "# hidden_size = 64\n",
    "# num_classes = len(label_encoder.classes_)\n",
    "# learning_rate = 0.001\n",
    "# num_epochs = 10\n",
    "\n",
    "# # Create an instance of the neural network model\n",
    "# model = NeuralNetwork(input_size, hidden_size, num_classes)\n",
    "\n",
    "# # Move the model to the device\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Define the loss function and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for batch_data, batch_targets in train_loader:\n",
    "#         # Move the batch data and targets to the device\n",
    "#         batch_data = batch_data.to(device)\n",
    "#         batch_targets = batch_targets.to(device)\n",
    "\n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         logits = model(batch_data)\n",
    "\n",
    "#         # Calculate the loss\n",
    "#         loss = criterion(logits, batch_targets)\n",
    "\n",
    "#         # Backward pass and optimization step\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # Evaluation\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     # Forward pass\n",
    "#     logits = model(x_test)\n",
    "\n",
    "#     # Get the predicted labels\n",
    "#     _, predicted = torch.max(logits.data, 1)\n",
    "\n",
    "#     # Calculate the accuracy\n",
    "#     accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "\n",
    "# print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7701644386608305\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import scipy.sparse as sp\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from torch import nn\n",
    "# from torch.optim import Adam\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Load the processed data\n",
    "# combined_sparse = sp.load_npz('data/combined_sparse.npz')\n",
    "# df_targets = pd.read_csv('data/df_targets.csv')\n",
    "# df_targets['PRIM_CONTRIBUTORY_CAUSE'] = pd.Categorical(df_targets['PRIM_CONTRIBUTORY_CAUSE'])\n",
    "\n",
    "# # Encode the target variable\n",
    "# label_encoder = LabelEncoder()\n",
    "# df_targets_encoded = pd.DataFrame()\n",
    "# df_targets_encoded['PRIM_CONTRIBUTORY_CAUSE_LABEL'] = label_encoder.fit_transform(df_targets['PRIM_CONTRIBUTORY_CAUSE'].cat.codes)\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#     combined_sparse,\n",
    "#     df_targets_encoded['PRIM_CONTRIBUTORY_CAUSE_LABEL'].values,\n",
    "#     test_size=0.2,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Convert the target arrays to PyTorch tensors\n",
    "# y_train_tensor = torch.tensor(y_train).long()\n",
    "# y_test_tensor = torch.tensor(y_test).long()\n",
    "\n",
    "# # Set the device to use for training\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Move the data to the device\n",
    "# x_train = torch.sparse_coo_tensor(x_train.nonzero(), x_train.data, x_train.shape, device=device).float()\n",
    "# x_test = torch.sparse_coo_tensor(x_test.nonzero(), x_test.data, x_test.shape, device=device).float()\n",
    "# y_train_tensor = y_train_tensor.to(device)\n",
    "# y_test_tensor = y_test_tensor.to(device)\n",
    "\n",
    "# # Create the training dataset and data loader\n",
    "# train_dataset = TensorDataset(x_train, y_train_tensor)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# # Define the neural network model\n",
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_classes):\n",
    "#         super(NeuralNetwork, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.fc1(x)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "\n",
    "# # Set the hyperparameters\n",
    "# input_size = combined_sparse.shape[1]\n",
    "# hidden_size = 128\n",
    "# num_classes = len(label_encoder.classes_)\n",
    "# learning_rate = 0.001\n",
    "# num_epochs = 10\n",
    "\n",
    "# # Create an instance of the neural network model\n",
    "# model = NeuralNetwork(input_size, hidden_size, num_classes)\n",
    "\n",
    "# # Move the model to the device\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Define the loss function and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for batch_data, batch_targets in train_loader:\n",
    "#         # Move the batch data and targets to the device\n",
    "#         batch_data = batch_data.to(device)\n",
    "#         batch_targets = batch_targets.to(device)\n",
    "\n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         logits = model(batch_data)\n",
    "\n",
    "#         # Calculate the loss\n",
    "#         loss = criterion(logits, batch_targets)\n",
    "\n",
    "#         # Backward pass and optimization step\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # Evaluation\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     # Forward pass\n",
    "#     logits = model(x_test)\n",
    "\n",
    "#     # Get the predicted labels\n",
    "#     _, predicted = torch.max(logits.data, 1)\n",
    "\n",
    "#     # Calculate the accuracy\n",
    "#     accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "\n",
    "# print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7796521944871643\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import scipy.sparse as sp\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from torch import nn\n",
    "# from torch.optim import Adam\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Load the processed data\n",
    "# combined_sparse = sp.load_npz('data/combined_sparse.npz')\n",
    "# df_targets = pd.read_csv('data/df_targets.csv')\n",
    "# df_targets['PRIM_CONTRIBUTORY_CAUSE'] = pd.Categorical(df_targets['PRIM_CONTRIBUTORY_CAUSE'])\n",
    "\n",
    "# # Encode the target variable\n",
    "# label_encoder = LabelEncoder()\n",
    "# df_targets_encoded = pd.DataFrame()\n",
    "# df_targets_encoded['PRIM_CONTRIBUTORY_CAUSE_LABEL'] = label_encoder.fit_transform(df_targets['PRIM_CONTRIBUTORY_CAUSE'].cat.codes)\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#     combined_sparse,\n",
    "#     df_targets_encoded['PRIM_CONTRIBUTORY_CAUSE_LABEL'].values,\n",
    "#     test_size=0.2,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Convert the target arrays to PyTorch tensors\n",
    "# y_train_tensor = torch.tensor(y_train).long()\n",
    "# y_test_tensor = torch.tensor(y_test).long()\n",
    "\n",
    "# # Set the device to use for training\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)\n",
    "# # Move the data to the device\n",
    "# x_train = torch.sparse_coo_tensor(x_train.nonzero(), x_train.data, x_train.shape, device=device).float()\n",
    "# x_test = torch.sparse_coo_tensor(x_test.nonzero(), x_test.data, x_test.shape, device=device).float()\n",
    "# y_train_tensor = y_train_tensor.to(device)\n",
    "# y_test_tensor = y_test_tensor.to(device)\n",
    "\n",
    "# # Create the training dataset and data loader\n",
    "# train_dataset = TensorDataset(x_train, y_train_tensor)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# # Define the neural network model\n",
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_classes):\n",
    "#         super(NeuralNetwork, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.fc1(x)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "\n",
    "# # Set the hyperparameters\n",
    "# input_size = combined_sparse.shape[1]\n",
    "# hidden_size = 128\n",
    "# num_classes = len(label_encoder.classes_)\n",
    "# learning_rate = 0.001\n",
    "# num_epochs = 10\n",
    "\n",
    "# # Create an instance of the neural network model\n",
    "# model = NeuralNetwork(input_size, hidden_size, num_classes)\n",
    "\n",
    "# # Move the model to the device\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Define the loss function and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for batch_data, batch_targets in train_loader:\n",
    "#         # Move the batch data and targets to the device\n",
    "#         batch_data = batch_data.to(device)\n",
    "#         batch_targets = batch_targets.to(device)\n",
    "\n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         logits = model(batch_data)\n",
    "\n",
    "#         # Calculate the loss\n",
    "#         loss = criterion(logits, batch_targets)\n",
    "\n",
    "#         # Backward pass and optimization step\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # Evaluation\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     # Forward pass\n",
    "#     logits = model(x_test)\n",
    "\n",
    "#     # Get the predicted labels\n",
    "#     _, predicted = torch.max(logits.data, 1)\n",
    "\n",
    "#     # Calculate the accuracy\n",
    "#     accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "\n",
    "# print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
